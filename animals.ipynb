{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from dataset to numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset directory\n",
    "dataset_dir = './datasets/animals/animals/'\n",
    "\n",
    "# All animal classes\n",
    "animal_classes = ['butterfly', 'cat', 'chicken', 'cow', 'dog', 'elephant', 'horse', 'sheep', 'spider', 'squirrel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Getting the list of all image files and their labels\n",
    "for class_name in animal_classes:\n",
    "    class_dir = os.path.join(dataset_dir, class_name)\n",
    "    # Getting the list of all image files of the current class\n",
    "    image_files = os.listdir(class_dir)\n",
    "    \n",
    "    # Processing all images of the current class\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(class_dir, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Changing the image size\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Adding the image and its corresponding label to the output lists\n",
    "        images.append(image)\n",
    "        labels.append(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the list of images and labels to numpy arrays\n",
    "le = preprocessing.LabelEncoder()\n",
    "targets = le.fit_transform(labels)\n",
    "\n",
    "images = np.array(images)\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training examples : \", len(X_train))\n",
    "print(\"Validation examples : \", len(X_valid))\n",
    "print(\"Testing examples : \", len(X_test))\n",
    "print(\"Image data shape : \", X_train.shape)\n",
    "print(\"Total classes : \", len(set(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_images_examples(image_array, grid_x, grid_y, title):\n",
    "    fig = plt.figure(figsize=(grid_x,grid_y))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    for i in range(1,grid_y*grid_x+1):\n",
    "        index = random.randint(0, len(image_array))\n",
    "        image = image_array[index].squeeze()\n",
    "\n",
    "        plt.subplot(grid_y,grid_x,i)\n",
    "        plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "n, bins, patches = plt.hist(y_train, len(set(y_train)))\n",
    "plt.xlabel('Class number')\n",
    "plt.ylabel('Amount of samples')\n",
    "plt.title('Training sample class distribution')\n",
    "\n",
    "draw_images_examples(X_train, 15, 3, 'Examples of images from training set')\n",
    "\n",
    "class_number = random.randint(0, len(y_train))\n",
    "example_class = y_train[class_number]\n",
    "\n",
    "X_train_one_label = X_train[np.where(y_train==example_class)]\n",
    "draw_images_examples(X_train_one_label, 15, 3, f'Examples of images of the class - {y_train[class_number]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, train dataset has class disbalance. Dogs and spiders have more data than other classes.\n",
    "Thus, we can use cut extra to make dataset more balanced.\n",
    "Thus, we can use data augmentation to make dataset more balanced.\n",
    "TODO:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my investigation I choose Convolutional Neural Networks as the widest, most efficient and classic models for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def to_device(X, y):\n",
    "    if (type(y) is tuple):\n",
    "        y = torch.as_tensor(targets)\n",
    "    return X.to(device), y.to(device, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X, self.y = X, y\n",
    "        self.count = len(self.y)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.X[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        return (X, self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader:\n",
    "    def __init__(self, loader, func):\n",
    "        self.loader = loader\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.loader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in iter(self.loader):\n",
    "            yield (self.func(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, gray=False):\n",
    "        super(LeNet, self).__init__()\n",
    "        input_channels = 1 if gray else 3\n",
    "        self.conv1 = nn.Conv2d(input_channels, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 43)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 4 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(model, loss_func, X, y, optimizer=None):\n",
    "    print('X.model:  ' + str(model(X).shape))\n",
    "    print('X.shape:  ' + str(X.shape))\n",
    "    print('y.shape:  ' + str(y.shape))\n",
    "\n",
    "    loss = loss_func(model(X), y)\n",
    "    if optimizer is not None:\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "    return loss.item(), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loss_func, X, y):\n",
    "    output = model(X)\n",
    "    loss = loss_func(output, y)\n",
    "    pred = torch.argmax(output, dim=1)\n",
    "    correct = pred == y.view(*pred.shape)\n",
    "\n",
    "    return loss.item(), torch.sum(correct).item(), len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loss_func, loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        validated_batches = []\n",
    "\n",
    "        for X, y in loader:\n",
    "          validated_batches.append(validate(model, loss_func, X, y))\n",
    "\n",
    "        losses, corrects, nums = zip(*validated_batches)\n",
    "        test_loss = sum(np.multiply(losses, nums)) / sum(nums)\n",
    "        test_accuracy = sum(corrects) / sum(nums) * 100\n",
    "\n",
    "    print(f\"Test loss: {test_loss:.5f}\\t\"\n",
    "          f\"Test accruacy: {test_accuracy:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_plots(losses_arr):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot([x[2] for x in losses_arr])\n",
    "    plt.ylabel('Accuracy in %')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.xticks([x + 1 for x in range(n_epochs) if x % 2 == 1])\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot([x[0] for x in losses_arr], label='train loss')\n",
    "    plt.plot([x[1] for x in losses_arr], label='validation loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel('Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.xticks([x + 1 for x in range(n_epochs) if x % 2 == 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, n_epochs, transforms, saving_model_path=None):\n",
    "    \n",
    "    train_dataset = TrafficSignDataset(X_train, y_train, transform=transforms)\n",
    "    valid_dataset = TrafficSignDataset(X_valid, y_valid, transform=transforms)\n",
    "    test_dataset = TrafficSignDataset(X_test, y_test, transform=transforms)\n",
    "\n",
    "    train_loader = WrappedDataLoader(DataLoader(train_dataset, batch_size=64, shuffle=True), to_device)\n",
    "    valid_loader = WrappedDataLoader(DataLoader(valid_dataset, batch_size=64, shuffle=False), to_device)\n",
    "    test_loader = WrappedDataLoader(DataLoader(test_dataset, batch_size=64, shuffle=False), to_device)\n",
    "\n",
    "    print('\\nFitting nn model')\n",
    "    start_time = time.time()\n",
    "\n",
    "    losses_arr = fit(n_epochs, model, criterion, optimizer, train_loader, valid_loader)\n",
    "    print(f'Fit time: {time.time() - start_time} s')\n",
    "\n",
    "    check_point = torch.load('model.pt', map_location=device)\n",
    "    model.load_state_dict(check_point)\n",
    "\n",
    "    evaluate(model, criterion, test_loader)\n",
    "\n",
    "    if saving_model_path is not None:\n",
    "        print('Saving model')\n",
    "        torch.save(model.state_dict(), model_path(saving_model_path))\n",
    "\n",
    "    training_plots(losses_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, channels, height, width = X_train.shape\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "model_path = lambda name: f\"./datasets/animals/animals/{name}.model\"\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, criterion, optimizer, n_epochs, transforms.ToTensor(), 'base_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
